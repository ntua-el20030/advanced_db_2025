%%configure -f
{
    "conf": {
        "spark.executor.instances": "4",
        "spark.executor.memory": "1g",
        "spark.executor.cores": "1",
        "spark.driver.memory": "2g"
    }
}

-----------------------------------------------------------------------

# Access configuration
conf = spark.sparkContext.getConf()

# Print relevant executor settings
print("Executor Instances:", conf.get("spark.executor.instances"))
print("Executor Memory:", conf.get("spark.executor.memory"))
print("Executor Cores:", conf.get("spark.executor.cores"))

-----------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count
import time

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Query 1 - Aggravated Assault Victims Age Groups") \
    .getOrCreate()

# Begin time calculation for DataFrame API
start_time_df = time.time()

# Load datasets with crime data from S3
crime_data_path_1_df = "s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv"
crime_data_2010_to_2019_df = spark.read.csv(crime_data_path_1_df, header=True, inferSchema=True)

crime_data_path_2_df = "s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv"
crime_data_2020_to_present_df = spark.read.csv(crime_data_path_2_df, header=True, inferSchema=True)

# Combine datasets
crime_data_df = crime_data_2010_to_2019_df.union(crime_data_2020_to_present_df)                                           
                                            
# Filter data for "aggravated assault"
filtered_data_df = crime_data_df.filter(col("Crm Cd Desc").like("%AGGRAVATED ASSAULT%"))

# Categorization of age groups
age_groups_df = filtered_data_df.withColumn(
    "Age_Group",
    when(col("Vict Age") < 18, "Children") \
    .when((col("Vict Age") >= 18) & (col("Vict Age") <= 24), "Young adults") \
    .when((col("Vict Age") >= 25) & (col("Vict Age") <= 64), "Adults") \
    .when(col("Vict Age") > 64, "Seniors") \
    .otherwise("Unknown")
)

# Grouping and counting
result_df = age_groups_df.groupBy("Age_Group").agg(count("*").alias("Count"))

# Sort in descending order
sorted_result_df = result_df.orderBy(col("Count").desc())

# Print the results
sorted_result_df.show()

# Stop counting execution time
end_time_df = time.time()

# Calculation of total execution time
execution_time_df = end_time_df - start_time_df
print(f"DataFrame API execution time: {execution_time_df} seconds")
print() 
print("-" * 80)

# Begin time calculation for RDD API
start_time_rdd = time.time()

# Load datasets with crime data from S3
crime_data_path_1_rdd = "s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv"
crime_data_2010_to_2019_rdd = spark.read.csv(crime_data_path_1_rdd, header=True, inferSchema=True).rdd

crime_data_path_2_rdd = "s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv"
crime_data_2020_to_present_rdd = spark.read.csv(crime_data_path_2_rdd, header=True, inferSchema=True).rdd

# Combine datasets
crime_data_rdd = crime_data_2010_to_2019_rdd.union(crime_data_2020_to_present_rdd)

# Filter data for "aggravated assault"
filtered_data_rdd = crime_data_rdd.filter(lambda row: "aggravated assault" in (row["Crm Cd Desc"] or "").lower())

# Categorization of age groups
def categorize_age(row):
    age = int(row["Vict Age"]) if row["Vict Age"] is not None else None
    if age is None:
        return "Unknown"
    elif age < 18:
        return "Children"
    elif 18 <= age <= 24:
        return "Young adults"
    elif 25 <= age <= 64:
        return "Adults"
    elif age > 64:
        return "Seniors"
    else:
        return "Unknown"

age_groups_rdd = filtered_data_rdd.map(lambda row: (categorize_age(row), 1))

# Grouping and counting
result_rdd = age_groups_rdd.reduceByKey(lambda a, b: a + b)

# Sort in descending order
sorted_result_rdd = result_rdd.sortBy(lambda x: x[1], ascending=False)

# Print the results
for group, count in result_rdd.collect():
    print(f"{group}: {count}")

print() 

# Stop counting execution time
end_time_rdd = time.time()

# Calculation of total execution time
execution_time_rdd = end_time_rdd - start_time_rdd
print(f"RDD API execution time: {execution_time_rdd} seconds")

# Stop SparkSession
spark.stop()
